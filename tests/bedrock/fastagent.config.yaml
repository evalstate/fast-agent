# Configuration file for Amazon Bedrock integration tests
# This file configures the fast-agent client for testing with Bedrock

# Default model to use when testing (updated to latest version)
default_model: "bedrock.us.anthropic.claude-3-7-sonnet-20250219-v1:0"

# Bedrock configuration
bedrock:
  # AWS region where Bedrock is available
  # Based on our model testing, us-east-1 or us-west-2 have the most comprehensive model support
  region: "us-east-1"
  
  # AWS credentials can be provided in multiple ways:
  # 1. Through environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
  # 2. Through AWS credentials file (~/.aws/credentials)
  # 3. Through IAM roles (when running on EC2 or ECS)
  
  # Option 1: Uncomment and set these values for direct API key authentication
  # access_key_id: "YOUR_AWS_ACCESS_KEY_ID"
  # secret_access_key: "YOUR_AWS_SECRET_ACCESS_KEY"
  
  # Option 2: Use AWS credential provider chain (environment variables, profiles, or IAM roles)
  use_default_credentials: true
  
  # Optional: AWS profile name from ~/.aws/credentials
  # profile: "bedrock-profile"
  
  # Optional: Custom endpoint URL (for VPC endpoints or testing)
  # endpoint_url: "https://bedrock-runtime.us-east-1.amazonaws.com"
  
  # Optional: Default parameters for all Bedrock models
  default_params:
    temperature: 0.7
    top_p: 0.9
    
  # Optional: Model-specific parameters
  model_params:
    # Claude 3.7 Sonnet
    "us.anthropic.claude-3-7-sonnet-20250219-v1:0":
      temperature: 0.7
      max_tokens: 4000
      top_p: 0.9
    
    # Claude 3.5 Sonnet (latest)
    "us.anthropic.claude-3-5-sonnet-20241022-v2:0":
      temperature: 0.7
      max_tokens: 4000
      top_p: 0.9
    
    # Claude 3.5 Sonnet (previous)
    "us.anthropic.claude-3-5-sonnet-20240620-v1:0":
      temperature: 0.7
      max_tokens: 4000
      top_p: 0.9
    
    # Claude 3 Haiku
    "us.anthropic.claude-3-haiku-20240307-v1:0":
      temperature: 0.7
      max_tokens: 2000
      top_p: 0.9

    # Nova models (Amazon's own models)
    "us.amazon.nova-lite-v1:0":
      temperature: 0.7
      maxTokenCount: 2048
      
    "us.amazon.nova-micro-v1:0":
      temperature: 0.7
      maxTokenCount: 4096
      
    "us.amazon.nova-pro-v1:0":
      temperature: 0.7
      maxTokenCount: 8192
      
    "us.amazon.nova-premier-v1:0":
      temperature: 0.7
      maxTokenCount: 16384
    
    # Meta models
    "us.meta.llama3-1-8b-instruct-v1:0":
      temperature: 0.7
      max_gen_len: 2048

# Logger settings
logging:
  level: "DEBUG"
  progress_display: true
  show_chat: true
  show_tools: true